{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate val and test F1 of a given model trained on algoritmh and hidden regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from algo_reasoning.src.models.network import EncodeProcessDecode\n",
    "from algo_reasoning.src.lightning.AlgorithmicReasoningTask import AlgorithmicReasoningTask \n",
    "from algo_reasoning.src.specs import CLRS_30_ALGS\n",
    "from algo_reasoning.src.losses.AlgorithmicReasoningLoss import AlgorithmicReasoningLoss\n",
    "from algo_reasoning.src.data import OriginalCLRSDataset, CLRSSampler, collate\n",
    "from algo_reasoning.src.sampler import CLRSDataset\n",
    "\n",
    "import lightning as L\n",
    "from torch.utils.data import DataLoader, get_worker_info\n",
    "from pathlib import Path\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_path = \"../checkpoints/\"\n",
    "\n",
    "def load_module(algorithm, hidden_reg):\n",
    "    dir_path = Path(f\"../checkpoints/{algorithm}/\")\n",
    "\n",
    "    ckpt_path = list(dir_path.glob(f\"*hidden_reg={hidden_reg}*\"))[0]\n",
    "\n",
    "    model = EncodeProcessDecode([algorithm])\n",
    "    loss_fn = AlgorithmicReasoningLoss(reg_weight=float(hidden_reg))\n",
    "    \n",
    "    model_hidden = AlgorithmicReasoningTask.load_from_checkpoint(ckpt_path, model=model, loss_fn=loss_fn)\n",
    "\n",
    "    return model_hidden\n",
    "\n",
    "def f1_val_test(algorithm, hidden_reg):\n",
    "    lightning_module = load_module(algorithm, hidden_reg)\n",
    "\n",
    "    trainer = L.Trainer(devices=1, accelerator=\"gpu\", use_distributed_sampler=False)\n",
    "\n",
    "    val_dataset = OriginalCLRSDataset([algorithm], \"val\", \"../tmp/CLRS30\")\n",
    "    val_sampler = CLRSSampler(val_dataset, algorithms=[algorithm], batch_size=8, seed=7)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_sampler=val_sampler, num_workers=8, persistent_workers=True, collate_fn=collate)\n",
    "    \n",
    "    test_dataset = OriginalCLRSDataset([algorithm], \"test\", \"../tmp/CLRS30\")\n",
    "    test_sampler = CLRSSampler(test_dataset, algorithms=[algorithm], batch_size=8, seed=7)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_sampler=test_sampler, num_workers=8, persistent_workers=True, collate_fn=collate)\n",
    "\n",
    "    val_metrics = trainer.test(lightning_module, val_dataloader, verbose=False)\n",
    "    test_metrics = trainer.test(lightning_module, test_dataloader, verbose=False)\n",
    "\n",
    "    return {\"val_metrics\": val_metrics[0][\"test_f1\"], \"test_metrics\":test_metrics[0][\"test_f1\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from algo_reasoning.src.specs import CLRS_30_ALGS\n",
    "import pandas as pd\n",
    "\n",
    "metrics_0 = []\n",
    "metrics_1 = []\n",
    "metrics_5 = []\n",
    "for algo in CLRS_30_ALGS:\n",
    "    print(algo)\n",
    "    metrics_0.append(f1_val_test(algo, \"0.0\")) \n",
    "    metrics_1.append(f1_val_test(algo, \"0.1\")) \n",
    "    metrics_5.append(f1_val_test(algo, \"0.5\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algos = pd.Series(CLRS_30_ALGS)\n",
    "\n",
    "metrics_0_val = pd.Series(metrics_0).map(lambda x: x[\"val_metrics\"])\n",
    "metrics_1_val = pd.Series(metrics_1).map(lambda x: x[\"val_metrics\"])\n",
    "metrics_5_val = pd.Series(metrics_5).map(lambda x: x[\"val_metrics\"])\n",
    "\n",
    "metrics_0_test = pd.Series(metrics_0).map(lambda x: x[\"test_metrics\"])\n",
    "metrics_1_test = pd.Series(metrics_1).map(lambda x: x[\"test_metrics\"])\n",
    "metrics_5_test = pd.Series(metrics_5).map(lambda x: x[\"test_metrics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_reg_df = pd.DataFrame({\"algorithm\": algos,\n",
    "                            \"0.0_val\": metrics_0_val,\n",
    "                             \"0.0_test\": metrics_0_test,\n",
    "                             \"0.1_val\": metrics_1_val,\n",
    "                             \"0.1_test\": metrics_1_test,\n",
    "                             \"0.5_val\": metrics_5_val,\n",
    "                             \"0.5_test\": metrics_5_test})\n",
    "hidden_reg_df = hidden_reg_df.set_index(\"algorithm\")\n",
    "hidden_reg_df.to_csv(\"../results/hidden_reg.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify effect of Hidden Similarity Regularization on Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "hidden_reg_df = pd.read_csv(\"../results/hidden_reg.csv\", delimiter=\";\")\n",
    "hidden_reg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing effect of Hidden Regularization to Algorithm Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_args = load_algorithm_args(\"../algorithm_args/default.yaml\")\n",
    "hidden_reg_df[\"max_length\"] = pd.Series([0] * len(hidden_reg_df))\n",
    "\n",
    "for alg in hidden_reg_df.algorithm:\n",
    "    print(\"Generating sample for: \", alg)\n",
    "    algorithms = [alg]\n",
    "    nb_nodes = 64\n",
    "    ds = CLRSDataset(algorithms, nb_nodes, 1, 1000, seed=7, algorithms_args=algorithm_args)\n",
    "    obj = next(iter(ds)).to(device=device)\n",
    "\n",
    "    hidden_reg_df.loc[hidden_reg_df.algorithm == alg, [\"max_length\"]] =  obj.max_length.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_1_effect = hidden_reg_df[\"0.1_test\"] - hidden_reg_df[\"0.0_test\"]\n",
    "reg_5_effect = hidden_reg_df[\"0.5_test\"] - hidden_reg_df[\"0.0_test\"]\n",
    "\n",
    "reg_0_ood_gap = hidden_reg_df[\"0.0_val\"] - hidden_reg_df[\"0.0_test\"]\n",
    "reg_1_ood_gap = hidden_reg_df[\"0.1_val\"] - hidden_reg_df[\"0.1_test\"]\n",
    "reg_5_ood_gap = hidden_reg_df[\"0.5_val\"] - hidden_reg_df[\"0.5_test\"]\n",
    "\n",
    "hidden_reg_df[\"reg_0.1_effect\"] = reg_1_effect\n",
    "hidden_reg_df[\"reg_0.5_effect\"] = reg_5_effect\n",
    "hidden_reg_df[\"reg_0.0_ood_gap\"] = reg_0_ood_gap\n",
    "hidden_reg_df[\"reg_0.1_ood_gap\"] = reg_1_ood_gap\n",
    "hidden_reg_df[\"reg_0.5_ood_gap\"] = reg_5_ood_gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_reg_df[[\"reg_0.1_effect\", \"reg_0.5_effect\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_corr = hidden_reg_df[hidden_reg_df.columns.difference(['algorithm'])].corr()\n",
    "\n",
    "_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect by Algorithm Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_dict = {\n",
    "    \"divide_and_conquer\": [\"find_maximum_subarray_kadane\"],\n",
    "    \"dynamic_programming\": [\"matrix_chain_order\", \"lcs_length\", \"optimal_bst\"],\n",
    "    \"geometry\": [\"segments_intersect\", \"graham_scan\", \"jarvis_march\"],\n",
    "    \"graphs\": [\"dfs\", \"bfs\", \"topological_sort\", \"articulation_points\", \"bridges\", \"strongly_connected_components\", \"mst_kruskal\", \"mst_prim\", \"bellman_ford\", \"dijkstra\", \"dag_shortest_paths\", \"floyd_warshall\"],\n",
    "    \"greedy\": [\"activity_selector\", \"task_scheduling\"], \n",
    "    \"searching\": [\"minimum\", \"binary_search\", \"quickselect\"],\n",
    "    \"sorting\": [\"insertion_sort\", \"bubble_sort\", \"heapsort\", \"quicksort\"],\n",
    "    \"strings\": [\"naive_string_matcher\", \"kmp_matcher\"]\n",
    "}\n",
    "\n",
    "def get_algo_type(algo):\n",
    "    for _type in type_dict.keys():\n",
    "        print\n",
    "        if algo in type_dict[_type]:\n",
    "            return _type\n",
    "        \n",
    "hidden_reg_df[\"_type\"] = hidden_reg_df.algorithm.apply(get_algo_type)\n",
    "agg_df = hidden_reg_df[hidden_reg_df.columns.difference(['algorithm'])].groupby(['_type']).mean()\n",
    "agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df[[\"reg_0.1_effect\", \"reg_0.5_effect\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at average OOD Gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_reg_df[[\"reg_0.0_ood_gap\", \"reg_0.1_ood_gap\", \"reg_0.5_ood_gap\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df[[\"reg_0.0_ood_gap\", \"reg_0.1_ood_gap\", \"reg_0.5_ood_gap\"]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
